huggingface å¥—ä»¶ä½¿ç”¨å¤‡å¿˜
===
<!--START_SECTION:badge-->

![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2023-06-25%2020%3A27%3A08&color=yellowgreen&style=flat-square)

<!--END_SECTION:badge-->
<!--info
top: false
hidden: false
-->

> ***Keywords**: huggingface*

<!--START_SECTION:toc-->
- [ç»„ä»¶](#ç»„ä»¶)
    - [transformers](#transformers)
    - [PEFT](#peft)
- [æ¡ˆä¾‹](#æ¡ˆä¾‹)
<!--END_SECTION:toc-->


## ç»„ä»¶
> [Hugging Face - Documentation](https://huggingface.co/docs)

### transformers
> æ¨¡åž‹åº“
- æ–‡æ¡£: [Transformers Doc](https://huggingface.co/docs/transformers/index)
- ä»“åº“: [huggingface/transformers: ðŸ¤— Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.](https://github.com/huggingface/transformers)


### PEFT
> Parameter-Efficient Fine-Tuning (PEFT)
>> è®­ç»ƒæ¨¡åž‹æ—¶åªå¾®è°ƒéƒ¨åˆ†å‚æ•°, å¸¸è§æ–¹æ³•: LoRA, P-Tuning, Prefix Tuning ç­‰;
- æ–‡æ¡£: [PEFT Doc](https://huggingface.co/docs/peft/index)
- ä»“åº“: [huggingface/peft: ðŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft)

LoRA: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS
Prefix Tuning: Prefix-Tuning: Optimizing Continuous Prompts for Generation, P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
P-Tuning: GPT Understands, Too
Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning
AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning

## æ¡ˆä¾‹
- [tloen/alpaca-lora: Instruct-tune LLaMA on consumer hardware](https://github.com/tloen/alpaca-lora)
    > ä½¿ç”¨ lora å¾®è°ƒ alpaca;

