语言模型
===
<!--START_SECTION:badge-->

![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2023-06-01%2015%3A06%3A05&color=yellowgreen&style=flat-square)

<!--END_SECTION:badge-->
<!--info
top: false
hidden: false
-->

> *keywords*: *语言模型 (Language Model)*

<!--START_SECTION:toc-->
- [概述](#概述)
    - [AR 与 AE 优缺点](#ar-与-ae-优缺点)
- [参考资料](#参考资料)
<!--END_SECTION:toc-->

<!-- 快速编辑

> algorithms/[xxx](../../../../algorithms/README.md#xxx)

<div align="center"><img src="../../../_assets/Sentence-BERT模型图.png" height="300" /></div>

-->


## 概述
- 语言模型通常指用来计算一个句子 (序列) 出现概率的模型; 
    - 记一个长度为 $n$ 的序列 $x=\left[x_0, x_1, .., x_{n-1}\right]$, 则语言模型的任务是建模联合概率 $P(x)=P(x_0, x_1, .., x_{n-1})$; 根据条件概率公式与链式法则, 该概率可以被分解为 
    $$
    P(x)=P(x_0)\prod \limits_{t=1}^{n-1} P(x_t|x_1, x_2, .., x_{t-1})=P(x_0)\prod \limits_{t=1}^{n-1} P(x_t|x_{<t})
    $$
    - 因为可以从 $P(x_t|x_1, x_2, .., x_{t-1})$ 反向计算出 $P(x)$, 所有有时也将语言模型直接定义为前者;
    > 一些文章将这种**从左向右**(根据前文预测下一个词)的语言模型称为 **自回归 (Auto-Regressive, AR) 语言模型**, 用来与 BERT 使用的 **自编码 (Auto-Encoder, AE) 语言模型** 区分;

### AR 与 AE 优缺点
> [XLNet:运行机制及和Bert的异同比较 - 知乎](https://zhuanlan.zhihu.com/p/70257427)
- AR
    - 优点
    - 缺点
- AE
    - 优点
    - 缺点


## 参考资料
- [XLNet:运行机制及和Bert的异同比较 - 知乎](https://zhuanlan.zhihu.com/p/70257427)