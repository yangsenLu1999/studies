不平衡学习专题
===
<!--START_SECTION:badge-->

![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2022-10-13%2001%3A56%3A19&color=yellowgreen&style=flat-square)

<!--END_SECTION:badge-->

- [不平衡问题简述](#不平衡问题简述)
- [基本方法](#基本方法)
    - [数据层面](#数据层面)
    - [算法层面](#算法层面)


## 不平衡问题简述

- 不平衡问题的本质是不同类别的**学习难易程度**不同，造成这个问题的原因是多样的；
- 最常见的一种原因是由于不同类别的训练样本**在数量上存在差异**。
    - 一般来说，某类别下的训练样本数量越多，该类别的学习效果就越好；这个问题一般可以在**训练前**就观察到；
    - 基于这个原因，一类方法（从**数据层面**）通过**调整不同类别的训练样本在数量上的分布**来缓解学习不平衡的问题，如**上采样**、**下采样**，及其改进方法；
    - 一个描述训练数据不平衡程度的参考指标：
        $$\rho = \frac{\max_i\{|C_i|\}}{\min_i\{|C_i|\}}$$
        > 其中 $|C_i|$ 表示 $i$ 类别样本的数量，比如最大类别的样本数为 100，最小的为 10，则 $\rho=10$；
- 另一方面，即使是在样本数量均衡的数据上训练也可能发生不平衡问题；
    - 造成这个现象的原因是因为学习器假设了**不同样本对模型的贡献是相同的**。但训练数据中可能存在**低质量样本**和**困难样本**等，这些样本显然不能一视同仁；而且这些问题**在训练之前观察不到**，需要在验证集上才能发现（通过混淆矩阵）；
    - 因此另一类方法（在**算法层面**）通过**调整不同类别样本的学习权重**来解决这个问题。这类方法的适用性更广，因为它在尝试从根源上解决不平衡问题；调整数据分布的方法本质上也是在调整学习权重，它的问题是忽略了样本自身的质量；


## 基本方法

### 数据层面

- 上采样（过采样，Over-Sampling）
- 下采样（欠采样，Under-Sampling）
- 两阶段学习（Two‑phase learning）
- 动态采样（Dynamic Sampling）


### 算法层面

- 阈值移动（Threshold Moving）
- 代价敏感学习（Cost Sensitive Learning）
- 通过修改 Loss
    - Focal Loss
- 基于聚类的方法
    - Category Centers
- 异常点检测（Outlier Detection）

