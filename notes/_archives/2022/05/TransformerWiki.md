Transformer Wiki
===
<!--START_SECTION:badge-->

![last modify](https://img.shields.io/static/v1?label=last%20modify&message=2023-02-02%2016%3A35%3A31&color=yellowgreen&style=flat-square)

<!--END_SECTION:badge-->
<!--info
top: false
hidden: false
-->

> ***Keywords**: transformer*

<!--START_SECTION:toc-->
- [背景](#背景)
- [常见 Transformer 变体](#常见-transformer-变体)
    - [长度外推性](#长度外推性)
- [常见面试问题](#常见面试问题)
<!--END_SECTION:toc-->


## 背景
- 原始 Transformer 指的是一个基于 Encoder-Decoder 框架的 Seq2Seq 模型，用于解决机器翻译任务；
- 后其 Encoder 部分被用于 BERT 而广为人知，因此有时 Transformer 也特指其 Encoder 部分；
- 相关论文：
    - [[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    - [[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

## 常见 Transformer 变体
> [Transformers系列模型](../10/Transformer系列模型.md)

### 长度外推性
> [Transformer与长度外推性](../../2023/02/Transformer与长度外推性.md)

## 常见面试问题
> [Transformer 常见面试问题](./Transformer常见问题.md)
